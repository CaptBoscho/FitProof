Pose landmark detection guide

A woman in a meditative pose. Her pose is highlighted with a wireframe that indicates the positioning of her limbs and torso

The MediaPipe Pose Landmarker task lets you detect landmarks of human bodies in an image or video. You can use this task to identify key body locations, analyze posture, and categorize movements. This task uses machine learning (ML) models that work with single images or video. The task outputs body pose landmarks in image coordinates and in 3-dimensional world coordinates.

Try it!arrow_forward

Get Started
Start using this task by following the implementation guide for your target platform. These platform-specific guides walk you through a basic implementation of this task, including a recommended model, and code example with recommended configuration options:

Android - Code example - Guide
Python - Code example - Guide
Web - Code example - Guide
Task details
This section describes the capabilities, inputs, outputs, and configuration options of this task.

Features
Input image processing - Processing includes image rotation, resizing, normalization, and color space conversion.
Score threshold - Filter results based on prediction scores.
Task inputs	Task outputs
The Pose Landmarker accepts an input of one of the following data types:
Still images
Decoded video frames
Live video feed
The Pose Landmarker outputs the following results:
Pose landmarks in normalized image coordinates
Pose landmarks in world coordinates
Optional: a segmentation mask for the pose.
Configurations options
This task has the following configuration options:

Option Name	Description	Value Range	Default Value
running_mode	Sets the running mode for the task. There are three modes:

IMAGE: The mode for single image inputs.

VIDEO: The mode for decoded frames of a video.

LIVE_STREAM: The mode for a livestream of input data, such as from a camera. In this mode, resultListener must be called to set up a listener to receive results asynchronously.	{IMAGE, VIDEO, LIVE_STREAM}	IMAGE
num_poses	The maximum number of poses that can be detected by the Pose Landmarker.	Integer > 0	1
min_pose_detection_confidence	The minimum confidence score for the pose detection to be considered successful.	Float [0.0,1.0]	0.5
min_pose_presence_confidence	The minimum confidence score of pose presence score in the pose landmark detection.	Float [0.0,1.0]	0.5
min_tracking_confidence	The minimum confidence score for the pose tracking to be considered successful.	Float [0.0,1.0]	0.5
output_segmentation_masks	Whether Pose Landmarker outputs a segmentation mask for the detected pose.	Boolean	False
result_callback	Sets the result listener to receive the landmarker results asynchronously when Pose Landmarker is in the live stream mode. Can only be used when running mode is set to LIVE_STREAM	ResultListener	N/A
Models
The Pose Landmarker uses a series of models to predict pose landmarks. The first model detects the presence of human bodies within an image frame, and the second model locates landmarks on the bodies.

The following models are packaged together into a downloadable model bundle:

Pose detection model: detects the presence of bodies with a few key pose landmarks.
Pose landmarker model: adds a complete mapping of the pose. The model outputs an estimate of 33 3-dimensional pose landmarks.
This bundle uses a convolutional neural network similar to MobileNetV2 and is optimized for on-device, real-time fitness applications. This variant of the BlazePose model uses GHUM, a 3D human shape modeling pipeline, to estimate the full 3D body pose of an individual in images or videos.

Attention: This MediaPipe Solutions Preview is an early release. Learn more.
Model bundle	Input shape	Data type	Model Cards	Versions
Pose landmarker (lite)	Pose detector: 224 x 224 x 3
Pose landmarker: 256 x 256 x 3	float 16	info	Latest
Pose landmarker (Full)	Pose detector: 224 x 224 x 3
Pose landmarker: 256 x 256 x 3	float 16	info	Latest
Pose landmarker (Heavy)	Pose detector: 224 x 224 x 3
Pose landmarker: 256 x 256 x 3	float 16	info	Latest
Pose landmarker model
The pose landmarker model tracks 33 body landmark locations, representing the approximate location of the following body parts:



0 - nose
1 - left eye (inner)
2 - left eye
3 - left eye (outer)
4 - right eye (inner)
5 - right eye
6 - right eye (outer)
7 - left ear
8 - right ear
9 - mouth (left)
10 - mouth (right)
11 - left shoulder
12 - right shoulder
13 - left elbow
14 - right elbow
15 - left wrist
16 - right wrist
17 - left pinky
18 - right pinky
19 - left index
20 - right index
21 - left thumb
22 - right thumb
23 - left hip
24 - right hip
25 - left knee
26 - right knee
27 - left ankle
28 - right ankle
29 - left heel
30 - right heel
31 - left foot index
32 - right foot index
The model output contains both normalized coordinates (Landmarks) and world coordinates (WorldLandmarks) for each landmark.

-----------------------

Pose landmark detection guide for Android

The MediaPipe Pose Landmarker task lets you detect landmarks of human bodies in an image or video. You can use this task to identify key body locations, analyze posture, and categorize movements. This task uses machine learning (ML) models that work with single images or video. The task outputs body pose landmarks in image coordinates and in 3-dimensional world coordinates.

The code sample described in these instructions is available on GitHub. For more information about the capabilities, models, and configuration options of this task, see the Overview.

Code example
The MediaPipe Tasks example code is a simple implementation of a Pose Landmarker app for Android. The example uses the camera on a physical Android device to detect poses in a continuous video stream. The app can also detect poses in images and videos from the device gallery.

You can use the app as a starting point for your own Android app, or refer to it when modifying an existing app. The Pose Landmarker example code is hosted on GitHub.

Download the code
The following instructions show you how to create a local copy of the example code using the git command line tool.

Attention: This MediaPipe Solutions Preview is an early release. Learn more.
To download the example code:

Clone the git repository using the following command:

git clone https://github.com/google-ai-edge/mediapipe-samples
Optionally, configure your git instance to use sparse checkout, so you have only the files for the Pose Landmarker example app:

cd mediapipe-samples
git sparse-checkout init --cone
git sparse-checkout set examples/pose_landmarker/android
After creating a local version of the example code, you can import the project into Android Studio and run the app. For instructions, see the Setup Guide for Android.

Key components
The following files contain the crucial code for this pose landmarking example application:

PoseLandmarkerHelper.kt - Initializes the pose landmarker and handles the model and delegate selection.
CameraFragment.kt - Handles the device camera and processes the image and video input data.
GalleryFragment.kt - Interacts with OverlayView to display the output image or video.
OverlayView.kt - Implements the display for the detected poses.
Setup
This section describes key steps for setting up your development environment and code projects specifically to use Pose Landmarker. For general information on setting up your development environment for using MediaPipe tasks, including platform version requirements, see the Setup guide for Android.

Attention: This MediaPipe Solutions Preview is an early release. Learn more.
Dependencies
The Pose Landmarker task uses the com.google.mediapipe:tasks-vision library. Add this dependency to the build.gradle file of your Android app:


dependencies {
    implementation 'com.google.mediapipe:tasks-vision:latest.release'
}
Model
The MediaPipe Pose Landmarker task requires a trained model bundle that is compatible with this task. For more information on available trained models for Pose Landmarker, see the task overview Models section.

Select and download the model, and store it within your project directory:


<dev-project-root>/src/main/assets
Specify the path of the model within the ModelAssetPath parameter. In the example code, the model is defined in the PoseLandmarkerHelper.kt file:


val modelName = "pose_landmarker_lite.task"
baseOptionsBuilder.setModelAssetPath(modelName)
Create the task
The MediaPipe Pose Landmarker task uses the createFromOptions() function to set up the task. The createFromOptions() function accepts values for the configuration options. For more information on configuration options, see Configuration options.

The Pose Landmarker supports the following input data types: still images, video files, and live video streams. You need to specify the running mode corresponding to your input data type when creating the task. Choose the tab for your input data type to see how to create the task.

Image
Video
Live stream

val baseOptionsBuilder = BaseOptions.builder().setModelAssetPath(modelName)
val baseOptions = baseOptionBuilder.build()

val optionsBuilder = 
    poseLandmarker.poseLandmarkerOptions.builder()
        .setBaseOptions(baseOptionsBuilder.build())
        .setMinPoseDetectionConfidence(minPoseDetectionConfidence)
        .setMinTrackingConfidence(minPoseTrackingConfidence)
        .setMinPosePresenceConfidence(minposePresenceConfidence)
        .setNumPoses(maxNumPoses)
        .setResultListener(this::returnLivestreamResult)
        .setErrorListener(this::returnLivestreamError)
        .setRunningMode(RunningMode.LIVE_STREAM)

val options = optionsBuilder.build()
poseLandmarker = poseLandmarker.createFromOptions(context, options)
    
Note: If you use the live stream mode, you’ll need to register a result listener when creating the task. The task calls the listener when it finishes processing a camera frame, with the detection result and the input image as parameters.
Note: If you use the video mode or live stream mode, Pose Landmarker uses tracking to avoid triggering palm detection model on every frame, which helps reduce latency.
The Pose Landmarker example code implementation allows the user to switch between processing modes. The approach makes the task creation code more complicated and may not be appropriate for your use case. You can see this code in the setupPoseLandmarker() function in the PoseLandmarkerHelper.kt file.

Configuration options
This task has the following configuration options for Android apps:

Option Name	Description	Value Range	Default Value
runningMode	Sets the running mode for the task. There are three modes:

IMAGE: The mode for single image inputs.

VIDEO: The mode for decoded frames of a video.

LIVE_STREAM: The mode for a livestream of input data, such as from a camera. In this mode, resultListener must be called to set up a listener to receive results asynchronously.	{IMAGE, VIDEO, LIVE_STREAM}	IMAGE
numposes	The maximum number of poses that can be detected by the Pose Landmarker.	Integer > 0	1
minPoseDetectionConfidence	The minimum confidence score for the pose detection to be considered successful.	Float [0.0,1.0]	0.5
minPosePresenceConfidence	The minimum confidence score of pose presence score in the pose landmark detection.	Float [0.0,1.0]	0.5
minTrackingConfidence	The minimum confidence score for the pose tracking to be considered successful.	Float [0.0,1.0]	0.5
outputSegmentationMasks	Whether Pose Landmarker outputs a segmentation mask for the detected pose.	Boolean	False
resultListener	Sets the result listener to receive the landmarker results asynchronously when Pose Landmarker is in the live stream mode. Can only be used when running mode is set to LIVE_STREAM	ResultListener	N/A
errorListener	Sets an optional error listener.	ErrorListener	N/A
Prepare data
Pose Landmarker works with images, video files, and live video streams. The task handles the data input preprocessing, including resizing, rotation and value normalization.

The following code demonstrates how to hand off data for processing. These samples include details on how to handle data from images, video files, and live video streams.

Image
Video
Live stream

import com.google.mediapipe.framework.image.BitmapImageBuilder
import com.google.mediapipe.framework.image.MPImage

// Convert the input Bitmap object to an MPImage object to run inference
val mpImage = BitmapImageBuilder(rotatedBitmap).build()
    
In the Pose Landmarker example code, the data preparation is handled in the PoseLandmarkerHelper.kt file.

Run the task
Depending on the type of data your are working with, use the poseLandmarker.detect...() method that is specific to that data type. Use detect() for individual images, detectForVideo() for frames in video files, and detectAsync() for video streams. When you are performing detections on a video stream, make sure you run the detections on a separate thread to avoid blocking the user interpose thread.

The following code samples show simple examples of how to run Pose Landmarker in these different data modes:

Image
Video
Live stream

val mpImage = BitmapImageBuilder(rotatedBitmap).build()
val frameTime = SystemClock.uptimeMillis()

poseLandmarker.detectAsync(mpImage, frameTime)
    
Note the following:

When running in the video mode or the live stream mode, you must provide the timestamp of the input frame to the Pose Landmarker task.
When running in the image or the video mode, the Pose Landmarker task blocks the current thread until it finishes processing the input image or frame. To avoid blocking the user interpose, execute the processing in a background thread.
When running in the live stream mode, the Pose Landmarker task returns immediately and doesn’t block the current thread. It will invoke the result listener with the detection result every time it finishes processing an input frame.
In the Pose Landmarker example code, the detect, detectForVideo, and detectAsync functions are defined in the PoseLandmarkerHelper.kt file.

Handle and display results
The Pose Landmarker returns a poseLandmarkerResult object for each detection run. The result object contains coordinates for each pose landmark.

The following shows an example of the output data from this task:


PoseLandmarkerResult:
  Landmarks:
    Landmark #0:
      x            : 0.638852
      y            : 0.671197
      z            : 0.129959
      visibility   : 0.9999997615814209
      presence     : 0.9999984502792358
    Landmark #1:
      x            : 0.634599
      y            : 0.536441
      z            : -0.06984
      visibility   : 0.999909
      presence     : 0.999958
    ... (33 landmarks per pose)
  WorldLandmarks:
    Landmark #0:
      x            : 0.067485
      y            : 0.031084
      z            : 0.055223
      visibility   : 0.9999997615814209
      presence     : 0.9999984502792358
    Landmark #1:
      x            : 0.063209
      y            : -0.00382
      z            : 0.020920
      visibility   : 0.999976
      presence     : 0.999998
    ... (33 world landmarks per pose)
  SegmentationMasks:
    ... (pictured below)
The output contains both normalized coordinates (Landmarks) and world coordinates (WorldLandmarks) for each landmark.

The output contains the following normalized coordinates (Landmarks):

x and y: Landmark coordinates normalized between 0.0 and 1.0 by the image width (x) and height (y).

z: The landmark depth, with the depth at the midpoint of the hips as the origin. The smaller the value, the closer the landmark is to the camera. The magnitude of z uses roughly the same scale as x.

visibility: The likelihood of the landmark being visible within the image.

The output contains the following world coordinates (WorldLandmarks):

x, y, and z: Real-world 3-dimensional coordinates in meters, with the midpoint of the hips as the origin.

visibility: The likelihood of the landmark being visible within the image.

The following image shows a visualization of the task output:

A woman in a meditative pose. Her pose is highlighted with a wireframe that indicates the positioning of her limbs and torso

The optional segmentation mask represents the likelihood of each pixel belonging to a detected person. The following image is a segmentation mask of the task output:

Segmentation mask of the previous image that outlines the shape of the woman

The Pose Landmarker example code demonstrates how to display the results returned from the task, see the OverlayView class for more details.

------------------------

Pose landmark detection guide for iOS

The Pose Landmarker task lets you detect landmarks of human bodies in an image or video. You can use this task to identify key body locations, analyze posture, and categorize movements. This task uses machine learning (ML) models that work with single images or video. The task outputs body pose landmarks in image coordinates and in 3-dimensional world coordinates.

These instructions show you how to use the Pose Landmarker with iOS apps. The code sample described in these instructions is available on GitHub.

You can see this task in action by viewing this Web demo. For more information about the capabilities, models, and configuration options of this task, see the Overview.

Code example
The MediaPipe Tasks example code is a basic implementation of a Pose Landmarker app for iOS. The example uses the camera on a physical iOS device to detect detect poses in a continuous video stream. The app can also detect poses in images and videos from the device gallery.

You can use the app as a starting point for your own iOS app, or refer to it when modifying an existing app. The Pose Landmarker example code is hosted on GitHub.

Download the code
The following instructions show you how to create a local copy of the example code using the git command line tool.

Attention: This MediaPipe Solutions Preview is an early release. Learn more.
To download the example code:

Clone the git repository using the following command:


git clone https://github.com/google-ai-edge/mediapipe-samples
Optionally, configure your git instance to use sparse checkout, so you have only the files for the Pose Landmarker example app:


cd mediapipe-samples
git sparse-checkout init --cone
git sparse-checkout set examples/pose_landmarker/ios/
After creating a local version of the example code, you can install the MediaPipe task library, open the project using Xcode and run the app. For instructions, see the Setup Guide for iOS.

Key components
The following files contain the crucial code for the Pose Landmarker example application:

PoseLandmarkerService.swift: Initializes the landmarker, handles the model selection and runs inference on the input data.
CameraViewController: Implements the UI for the live camera feed input mode and visualizes the landmarks.
MediaLibraryViewController.swift: Implements the UI for the still image and video file input mode and visualizes the landmarks.
Setup
This section describes key steps for setting up your development environment and code projects to use Pose Landmarker. For general information on setting up your development environment for using MediaPipe tasks, including platform version requirements, see the Setup guide for iOS.

Attention: This MediaPipe Solutions Preview is an early release. Learn more.
Dependencies
Pose Landmarker uses the MediaPipeTasksVision library, which must be installed using CocoaPods. The library is compatible with both Swift and Objective-C apps and does not require any additional language-specific setup.

For instructions to install CocoaPods on macOS, refer to the CocoaPods installation guide. For instructions on how to create a Podfile with the necessary pods for your app, refer to Using CocoaPods.

Add the MediaPipeTasksVision pod in the Podfile using the following code:


target 'MyPoseLandmarkerApp' do
  use_frameworks!
  pod 'MediaPipeTasksVision'
end
If your app includes unit test targets, refer to the Set Up Guide for iOS for additional information on setting up your Podfile.

Note: The following sections provide code examples in both Swift and Objective-C. When applicable, the explanations refer to the Swift method names.
Model
The MediaPipe Pose Landmarker task requires a trained bundle that is compatible with this task. For more information on available trained models for Pose Landmarker, see the task overview Models section.

Use the download_models.sh script to download the models and add it to your project directory using Xcode. For instructions on how to add files to your Xcode project, refer to Managing files and folders in your Xcode project.

Use the BaseOptions.modelAssetPath property to specify the path to the model in your app bundle. For a code example, see the next section.

Create the task
You can create the Pose Landmarker task by calling one of its initializers. The PoseLandmarker(options:) initializer accepts values for the configuration options.

If you don't need a Pose Landmarker initialized with customized configuration options, you can use the PoseLandmarker(modelPath:) initializer to create an Pose Landmarker with the default options. For more information about configuration options, see Configuration Overview.

The Pose Landmarker task supports 3 input data types: still images, video files and live video streams. By default, PoseLandmarker(modelPath:) initializes a task for still images. If you want your task to be initialized to process video files or live video streams, use PoseLandmarker(options:) to specify the video or livestream running mode. The livestream mode also requires the additional poseLandmarkerLiveStreamDelegate configuration option, which enables the Pose Landmarker to deliver the pose landmark detection results to the delegate asynchronously.

Choose the tab corresponding to your running mode to see how to create the task and run inference.

Swift
Image
Video
Livestream

import MediaPipeTasksVision

let modelPath = Bundle.main.path(forResource: "pose_landmarker",
                                      ofType: "task")

let options = PoseLandmarkerOptions()
options.baseOptions.modelAssetPath = modelPath
options.runningMode = .image
options.minPoseDetectionConfidence = minPoseDetectionConfidence
options.minPosePresenceConfidence = minPosePresenceConfidence
options.minTrackingConfidence = minTrackingConfidence
options.numPoses = numPoses

let poseLandmarker = try PoseLandmarker(options: options)
    
Objective-C
Image
Video
Livestream

@import MediaPipeTasksVision;

NSString *modelPath = [[NSBundle mainBundle] pathForResource:@"pose_landmarker"
                                                      ofType:@"task"];

MPPPoseLandmarkerOptions *options = [[MPPPoseLandmarkerOptions alloc] init];
options.baseOptions.modelAssetPath = modelPath;
options.runningMode = MPPRunningModeImage;
options.minPoseDetectionConfidence = minPoseDetectionConfidence;
options.minPosePresenceConfidence = minPosePresenceConfidence;
options.minTrackingConfidence = minTrackingConfidence;
options.numPoses = numPoses;

MPPPoseLandmarker *poseLandmarker =
  [[MPPPoseLandmarker alloc] initWithOptions:options error:nil];
    
Note: If you use the video mode or livestream mode, Pose Landmarker uses tracking to avoid triggering palm detection model on every frame, which helps reduce latency.

Configuration options
This task has the following configuration options for iOS apps:

Option Name	Description	Value Range	Default Value
running_mode	Sets the running mode for the task. There are three modes:

IMAGE: The mode for single image inputs.

VIDEO: The mode for decoded frames of a video.

LIVE_STREAM: The mode for a livestream of input data, such as from a camera. In this mode, poseLandmarkerLiveStreamDelegate must be set to an instance of a class that implements the PoseLandmarkerLiveStreamDelegate to receive the results of performing pose landmark detection asynchronously.	{RunningMode.image, RunningMode.video, RunningMode.liveStream}	RunningMode.image
num_poses	The maximum number of poses that can be detected by the Pose Landmarker.	Integer > 0	1
min_pose_detection_confidence	The minimum confidence score for the pose detection to be considered successful.	Float [0.0,1.0]	0.5
min_pose_presence_confidence	The minimum confidence score of pose presence score in the pose landmark detection.	Float [0.0,1.0]	0.5
min_tracking_confidence	The minimum confidence score for the pose tracking to be considered successful.	Float [0.0,1.0]	0.5
output_segmentation_masks	Whether Pose Landmarker outputs a segmentation mask for the detected pose.	Boolean	False
result_callback	Sets the result listener to receive the landmarker results asynchronously when Pose Landmarker is in the live stream mode. Can only be used when running mode is set to LIVE_STREAM	ResultListener	N/A
Livestream configuration
When the running mode is set to livestream, the Pose Landmarker requires the additional poseLandmarkerLiveStreamDelegate configuration option, which enables the Pose Landmarker to deliver pose landmark detection results asynchronously. The delegate must implement the poseLandmarker(_:didFinishDetection:timestampInMilliseconds:error:) method, which the Pose Landmarker calls after processing the results of performing pose landmark detection on each frame.

Option name	Description	Value Range	Default Value
poseLandmarkerLiveStreamDelegate	Enables Pose Landmarker to receive the results of performing pose landmark detection asynchronously in livestream mode. The class whose instance is set to this property must implement the poseLandmarker(_:didFinishDetection:timestampInMilliseconds:error:) method.	Not applicable	Not set
Prepare data
You need to convert the input image or frame to an MPImage object before passing it to the Pose Landmarker. MPImage supports different types of iOS image formats, and can use them in any running mode for inference. For more information about MPImage, refer to the MPImage API.

Choose an iOS image format based on your use case and the running mode your application requires.MPImage accepts the UIImage, CVPixelBuffer, and CMSampleBuffer iOS image formats.

UIImage
The UIImage format is well-suited for the following running modes:

Images: images from an app bundle, user gallery, or file system formatted as UIImage images can be converted to an MPImage object.

Videos: use AVAssetImageGenerator to extract video frames to the CGImage format, then convert them to UIImage images.

Swift
Objective-C

// Load an image on the user's device as an iOS `UIImage` object.

// Convert the `UIImage` object to a MediaPipe's Image object having the default
// orientation `UIImage.Orientation.up`.
let image = try MPImage(uiImage: image)
    
The example initializes an MPImage with the default UIImage.Orientation.Up orientation. You can initialize an MPImage with any of the supported UIImage.Orientation values. Pose Landmarker does not support mirrored orientations like .upMirrored, .downMirrored, .leftMirrored, .rightMirrored.

For more information about UIImage, refer to the UIImage Apple Developer Documentation.

CVPixelBuffer
The CVPixelBuffer format is well-suited for applications that generate frames and use the iOS CoreImage framework for processing.

The CVPixelBuffer format is well-suited for the following running modes:

Images: apps that generate CVPixelBuffer images after some processing using iOS's CoreImage framework can be sent to the Pose Landmarker in the image running mode.

Videos: video frames can be converted to the CVPixelBuffer format for processing, and then sent to the Pose Landmarker in video mode.

livestream: apps using an iOS camera to generate frames may be converted into the CVPixelBuffer format for processing before being sent to the Pose Landmarker in livestream mode.

Swift
Objective-C

// Obtain a CVPixelBuffer.

// Convert the `CVPixelBuffer` object to a MediaPipe's Image object having the default
// orientation `UIImage.Orientation.up`.
let image = try MPImage(pixelBuffer: pixelBuffer)
    
For more information about CVPixelBuffer, refer to the CVPixelBuffer Apple Developer Documentation.

CMSampleBuffer
The CMSampleBuffer format stores media samples of a uniform media type, and is well-suited for the livestream running mode. Live frames from iOS cameras are asynchronously delivered in the CMSampleBuffer format by iOS AVCaptureVideoDataOutput.

Swift
Objective-C

// Obtain a CMSampleBuffer.

// Convert the `CMSampleBuffer` object to a MediaPipe's Image object having the default
// orientation `UIImage.Orientation.up`.
let image = try MPImage(sampleBuffer: sampleBuffer)
    
For more information about CMSampleBuffer, refer to the CMSampleBuffer Apple Developer Documentation.

Run the task
To run the Pose Landmarker, use the detect() method specific to the assigned running mode:

Still image: detect(image:)
Video: detect(videoFrame:timestampInMilliseconds:)
Livestream: detectAsync(image:timestampInMilliseconds:)
The following code samples show simple examples of how to run Pose Landmarker in these different running modes:

Swift
Image
Video
Livestream

let result = try poseLandmarker.detect(image: image)
    
Objective-C
Image
Video
Livestream

MPPPoseLandmarkerResult *result =
  [poseLandmarker detectImage:image error:nil];
    
The Pose Landmarker code example shows the implementations of each of these modes in more detail detect(image:), detect(videoFrame:timestampInMilliseconds:), and detectAsync(image:timestampInMilliseconds:). The example code allows the user to switch between processing modes which may not be required for your use case.

Note the following:

When running in video mode or livestream mode, you must also provide the timestamp of the input frame to the Pose Landmarker task.

When running in image or video mode, the Pose Landmarker task blocks the current thread until it finishes processing the input image or frame. To avoid blocking the current thread, execute the processing in a background thread using iOS Dispatch or NSOperation frameworks.

When running in livestream mode, the Pose Landmarker task returns immediately and doesn't block the current thread. It invokes the poseLandmarker(_:didFinishDetection:timestampInMilliseconds:error:) method with the pose landmarker result after processing each input frame. The Pose Landmarker invokes this method asynchronously on a dedicated serial dispatch queue. For displaying results on the user interface, dispatch the results to the main queue after processing the results. If the detectAsync function is called when the Pose Landmarker task is busy processing another frame, the Pose Landmarker ignores the new input frame.

Handle and display results
Upon running inference, the Pose Landmarker task returns a PoseLandmarkerResult which contains the coordinates for each pose landmark.

The following shows an example of the output data from this task:


PoseLandmarkerResult:
  Landmarks:
    Landmark #0:
      x            : 0.638852
      y            : 0.671197
      z            : 0.129959
      visibility   : 0.9999997615814209
      presence     : 0.9999984502792358
    Landmark #1:
      x            : 0.634599
      y            : 0.536441
      z            : -0.06984
      visibility   : 0.999909
      presence     : 0.999958
    ... (33 landmarks per pose)
  WorldLandmarks:
    Landmark #0:
      x            : 0.067485
      y            : 0.031084
      z            : 0.055223
      visibility   : 0.9999997615814209
      presence     : 0.9999984502792358
    Landmark #1:
      x            : 0.063209
      y            : -0.00382
      z            : 0.020920
      visibility   : 0.999976
      presence     : 0.999998
    ... (33 world landmarks per pose)
  SegmentationMasks:
    ... (pictured below)
The output contains both normalized coordinates (Landmarks) and world coordinates (WorldLandmarks) for each landmark.

The output contains the following normalized coordinates (Landmarks):

x and y: Landmark coordinates normalized between 0.0 and 1.0 by the image width (x) and height (y).

z: The landmark depth, with the depth at the midpoint of the hips as the origin. The smaller the value, the closer the landmark is to the camera. The magnitude of z uses roughly the same scale as x.

visibility: The likelihood of the landmark being visible within the image.

The output contains the following world coordinates (WorldLandmarks):

x, y, and z: Real-world 3-dimensional coordinates in meters, with the midpoint of the hips as the origin.

visibility: The likelihood of the landmark being visible within the image.

The following image shows a visualization of the task output:

A woman in a meditative pose. Her pose is highlighted with a wireframe that indicates the positioning of her limbs and torso

The optional segmentation mask represents the likelihood of each pixel belonging to a detected person. The following image is a segmentation mask of the task output:

Segmentation mask of the previous image that outlines the shape of the woman

The Pose Landmarker example code demonstrates how to display the Pose Landmarker results.

------------------------

Pose landmark detection guide for Web

The MediaPipe Pose Landmarker task lets you detect landmarks of human bodies in an image or video. You can use this task to identify key body locations, analyze posture, and categorize movements. This task uses machine learning (ML) models that work with single images or video. The task outputs body pose landmarks in image coordinates and in 3-dimensional world coordinates.

These instructions show you how to use the Pose Landmarker for web and JavaScript apps. For more information about the capabilities, models, and configuration options of this task, see the Overview.

Code example
The example code for Pose Landmarker provides a complete implementation of this task in JavaScript for your reference. This code helps you test this task and get started on building your own pose landmarker app. You can view, run, and edit the Pose Landmarker example code using just your web browser.

Setup
This section describes key steps for setting up your development environment specifically to use Pose Landmarker. For general information on setting up your web and JavaScript development environment, including platform version requirements, see the Setup guide for web.

JavaScript packages
Pose Landmarker code is available through the MediaPipe @mediapipe/tasks-vision NPM package. You can find and download these libraries by following the instructions in the platform Setup guide.

Attention: This MediaPipe Solutions Preview is an early release. Learn more.
You can install the required packages through NPM using the following command:


npm install @mediapipe/tasks-vision
If you want to import the task code via a content delivery network (CDN) service, add the following code in the <head> tag in your HTML file:


<!-- You can replace JSDeliver with another CDN if you prefer -->
<head>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/vision_bundle.js"
    crossorigin="anonymous"></script>
</head>
Model
The MediaPipe Pose Landmarker task requires a trained model that is compatible with this task. For more information on available trained models for Pose Landmarker, see the task overview Models section.

Select and download a model, and then store it within your project directory:


<dev-project-root>/app/shared/models/
Create the task
Use one of the Pose Landmarker createFrom...() functions to prepare the task for running inferences. Use the createFromModelPath() function with a relative or absolute path to the trained model file. If your model is already loaded into memory, you can use the createFromModelBuffer() method.

The code example below demonstrates using the createFromOptions() function to set up the task. The createFromOptions() function allows you to customize the Pose Landmarker with configuration options. For more information on configuration options, see Configuration options.

The following code demonstrates how to build and configure the task with custom options:


const vision = await FilesetResolver.forVisionTasks(
  // path/to/wasm/root
  "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm"
);
const poseLandmarker = await poseLandmarker.createFromOptions(
    vision,
    {
      baseOptions: {
        modelAssetPath: "path/to/model"
      },
      runningMode: runningMode
    });
Configuration options
This task has the following configuration options for Web and JavaScript applications:

Option Name	Description	Value Range	Default Value
runningMode	Sets the running mode for the task. There are two modes:

IMAGE: The mode for single image inputs.

VIDEO: The mode for decoded frames of a video or on a livestream of input data, such as from a camera.	{IMAGE, VIDEO}	IMAGE
numPoses	The maximum number of poses that can be detected by the Pose Landmarker.	Integer > 0	1
minPoseDetectionConfidence	The minimum confidence score for the pose detection to be considered successful.	Float [0.0,1.0]	0.5
minPosePresenceConfidence	The minimum confidence score of pose presence score in the pose landmark detection.	Float [0.0,1.0]	0.5
minTrackingConfidence	The minimum confidence score for the pose tracking to be considered successful.	Float [0.0,1.0]	0.5
outputSegmentationMasks	Whether Pose Landmarker outputs a segmentation mask for the detected pose.	Boolean	False
Prepare data
Pose Landmarker can detect poses in images in any format supported by the host browser. The task also handles data input preprocessing, including resizing, rotation and value normalization. To landmark poses in videos, you can use the API to quickly process one frame at a time, using the timestamp of the frame to determine when the poses occur within the video.

Run the task
The Pose Landmarker uses the detect() (with running mode IMAGE) and detectForVideo() (with running mode VIDEO) methods to trigger inferences. The task processes the data, attempts to landmark poses, and then reports the results.

Calls to the Pose Landmarker detect() and detectForVideo() methods run synchronously and block the user interpose thread. If you detect poses in video frames from a device's camera, each detection blocks the main thread. You can prevent this by implementing web workers to run the detect() and detectForVideo() methods on another thread.

The following code demonstrates how execute the processing with the task model:

Image
Video

await poseLandmarker.setOptions({ runningMode: "VIDEO" });

let lastVideoTime = -1;
function renderLoop(): void {
  const video = document.getElementById("video");

  if (video.currentTime !== lastVideoTime) {
    const poseLandmarkerResult = poseLandmarker.detectForVideo(video);
    processResults(detections);
    lastVideoTime = video.currentTime;
  }

  requestAnimationFrame(() => {
    renderLoop();
  });
}
For a more complete implementation of running an Pose Landmarker task, see the code example.

Handle and display results
The Pose Landmarker returns a poseLandmarkerResult object for each detection run. The result object contains coordinates for each pose landmark.

The following shows an example of the output data from this task:


PoseLandmarkerResult:
  Landmarks:
    Landmark #0:
      x            : 0.638852
      y            : 0.671197
      z            : 0.129959
      visibility   : 0.9999997615814209
      presence     : 0.9999984502792358
    Landmark #1:
      x            : 0.634599
      y            : 0.536441
      z            : -0.06984
      visibility   : 0.999909
      presence     : 0.999958
    ... (33 landmarks per pose)
  WorldLandmarks:
    Landmark #0:
      x            : 0.067485
      y            : 0.031084
      z            : 0.055223
      visibility   : 0.9999997615814209
      presence     : 0.9999984502792358
    Landmark #1:
      x            : 0.063209
      y            : -0.00382
      z            : 0.020920
      visibility   : 0.999976
      presence     : 0.999998
    ... (33 world landmarks per pose)
  SegmentationMasks:
    ... (pictured below)
The output contains both normalized coordinates (Landmarks) and world coordinates (WorldLandmarks) for each landmark.

The output contains the following normalized coordinates (Landmarks):

x and y: Landmark coordinates normalized between 0.0 and 1.0 by the image width (x) and height (y).

z: The landmark depth, with the depth at the midpoint of the hips as the origin. The smaller the value, the closer the landmark is to the camera. The magnitude of z uses roughly the same scale as x.

visibility: The likelihood of the landmark being visible within the image.

The output contains the following world coordinates (WorldLandmarks):

x, y, and z: Real-world 3-dimensional coordinates in meters, with the midpoint of the hips as the origin.

visibility: The likelihood of the landmark being visible within the image.

The following image shows a visualization of the task output:

A woman in a meditative pose. Her pose is highlighted with a wireframe that indicates the positioning of her limbs and torso

The optional segmentation mask represents the likelihood of each pixel belonging to a detected person. The following image is a segmentation mask of the task output:

Segmentation mask of the previous image that outlines the shape of the woman

The Pose Landmarker example code demonstrates how to display the results returned from the task, see the code example
